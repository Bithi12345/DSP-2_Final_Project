{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Features: ['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company', 'Work_accident', 'promotion_last_5years', 'Departments', 'salary', 'work_hours_per_project']\n",
      "Categorical Features: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load data\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "\n",
    "# Remove duplicates\n",
    "train_data = train_data.drop_duplicates()\n",
    "\n",
    "# Encode categorical variables manually (salary levels)\n",
    "salary_mapping = {\"low\": 0, \"medium\": 1, \"high\": 2}\n",
    "train_data[\"salary\"] = train_data[\"salary\"].map(salary_mapping)\n",
    "\n",
    "# Define features and target variable\n",
    "X = train_data.drop(\"left\", axis=1)\n",
    "y = train_data[\"left\"].copy()\n",
    "\n",
    "# Splitting dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Identify numerical and categorical features\n",
    "num_features = X_train.select_dtypes(include=np.number).columns\n",
    "cat_features = X_train.select_dtypes(include=\"object\").columns\n",
    "\n",
    "print(f\"Numerical Features: {list(num_features)}\")\n",
    "print(f\"Categorical Features: {list(cat_features)}\")\n",
    "\n",
    "# Save train and validation sets\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "X_train.to_csv(\"data/X_train.csv\", index=False)\n",
    "X_val.to_csv(\"data/X_val.csv\", index=False)\n",
    "y_train.to_csv(\"data/y_train.csv\", index=False)\n",
    "y_val.to_csv(\"data/y_val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColumnTransformer(transformers=[('num_pipe',\n",
      "                                 Pipeline(steps=[('num_imputer',\n",
      "                                                  SimpleImputer()),\n",
      "                                                 ('std_scaler',\n",
      "                                                  StandardScaler())]),\n",
      "                                 Index(['satisfaction_level', 'last_evaluation', 'number_project',\n",
      "       'average_montly_hours', 'time_spend_company', 'Work_accident',\n",
      "       'promotion_last_5years', 'Departments', 'salary',\n",
      "       'work_hours_per_project'],\n",
      "      dtype='object')),\n",
      "                                ('cat_pipe',\n",
      "                                 Pipeline(steps=[('cat_imputer',\n",
      "                                                  SimpleImputer(strategy='most_frequent')),\n",
      "                                                 ('ordinal_encoder',\n",
      "                                                  OrdinalEncoder())]),\n",
      "                                 Index([], dtype='object'))])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Numerical pipeline: handle missing values and scale data\n",
    "num_pipeline = Pipeline([\n",
    "    (\"num_imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"std_scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline: handle missing values and encode categorical data\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"cat_imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ordinal_encoder\", OrdinalEncoder())\n",
    "])\n",
    "\n",
    "# Combine both pipelines using ColumnTransformer\n",
    "pre_processing_pipeline = ColumnTransformer([\n",
    "    (\"num_pipe\", num_pipeline, num_features),\n",
    "    (\"cat_pipe\", cat_pipeline, cat_features)\n",
    "])\n",
    "\n",
    "print(pre_processing_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('pre_processing',\n",
      "                 ColumnTransformer(transformers=[('num_pipe',\n",
      "                                                  Pipeline(steps=[('num_imputer',\n",
      "                                                                   SimpleImputer()),\n",
      "                                                                  ('std_scaler',\n",
      "                                                                   StandardScaler())]),\n",
      "                                                  Index(['satisfaction_level', 'last_evaluation', 'number_project',\n",
      "       'average_montly_hours', 'time_spend_company', 'Work_accident',\n",
      "       'promotion_last_5years', 'Departments', 'salary',\n",
      "       'work_hours_per_project'],\n",
      "      dtype='object')),\n",
      "                                                 ('cat_pipe',\n",
      "                                                  Pipeline(steps=[('cat_imputer',\n",
      "                                                                   SimpleImputer(strategy='most_frequent')),\n",
      "                                                                  ('ordinal_encoder',\n",
      "                                                                   OrdinalEncoder())]),\n",
      "                                                  Index([], dtype='object'))])),\n",
      "                ('model',\n",
      "                 RandomForestClassifier(n_estimators=120, random_state=42))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DSP-2_Final_Project\\venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['salary']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a complete ML pipeline with preprocessing and model\n",
    "model_pipeline = Pipeline([\n",
    "    (\"pre_processing\", pre_processing_pipeline),\n",
    "    (\"model\", RandomForestClassifier(n_estimators=120, random_state=42))\n",
    "])\n",
    "\n",
    "print(model_pipeline)\n",
    "\n",
    "# Train the model\n",
    "model = model_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\DSP-2_Final_Project\\venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['salary']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "joblib.dump(model, \"models/hr_model_pipeline.pkl\")\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
